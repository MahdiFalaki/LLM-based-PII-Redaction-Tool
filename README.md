# üîí Text Redaction with Mistral-7B 

This project fine-tunes [mistralai/Mistral-7B-Instruct-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2) on the [ai4privacy/pii-masking-200k](https://huggingface.co/datasets/ai4privacy/pii-masking-200k) dataset to **redact sensitive information (PII)**. It supports both **GPU inference (merged Hugging Face model)** and **CPU inference demo-only (quantized GGUF model via llama.cpp)**, with post-processing for consistent normalization of entity tags. 

--- 

## üöÄ Motivation 

Handling sensitive data securely is critical. Off-the-shelf LLMs are not optimized for **structured text redaction** (names, addresses, phone numbers, credit cards, etc.). This project builds a reproducible pipeline for **finetuning, merging, quantization, inference, and evaluation**, producing a lightweight model ready for real-world redaction tasks. 

--- 

## üîÑ Workflow 

1. **Base Model Selection**

   Use mistralai/Mistral-7B-Instruct-v0.2 as a strong multilingual instruction-following base.

2. **Dataset Conversion** Convert ai4privacy/pii-masking-200k into **Alpaca-style JSONL** for Axolotl:
```
   {
     "system": "You are a data privacy assistant.",
     "instruction": "Mask all personally identifiable information in the given text.",
     "input": "John Smith lives at 123 Main Street, Toronto.",
     "output": "[FIRSTNAME] [LASTNAME] lives at [ADDRESS]."
   }
```
3. **Configuration & Finetuning**
   
   Define hyperparameters in config/pii_config.yml and fine-tune with Axolotl.

4. **LoRA Merge**
   
   Merge LoRA adapter weights into the base model to produce a standalone merged model.

5. **Post-Processing**

   * Canonicalize tags (e.g., [Firstname] ‚Üí [FIRSTNAME])

   * Collapse structured blocks ([BUILDINGNUMBER] [STREET] [CITY] ‚Üí [ADDRESS])

   * Override credit card patterns if misclassified as phone numbers

   * Reduce tag space for quantized model (e.g., CITY/STATE ‚Üí [ADDRESS])

6. **Inference**

   * GPU (HF Transformers): run inference on merged model

   * CPU (llama.cpp): convert to GGUF, quantize (Q4_K_M), and run on CPU

7. **Evaluation**
   
   Compare GPU vs CPU outputs on 100 random samples using scripts/eval/run_eval.py. Metrics include confusion matrices and macro Precision/Recall/F1.

**üìÇ Repository Layout**

```
${PROJECT_ROOT}

   -- config
     |-- pii_config.yml
     |-- __init__.py
     |-- config.py

   -- data
     |-- pii_mask.jsonl -- merged-gguf
     |-- mistral7b-redact-f16.gguf
     |-- mistral7b-redact-Q4_K_M.gguf

   -- scripts

     -- train
       |-- convert_pii_dataset.py
       |-- axolotl_train.py

     -- infer
       |-- gguf_infer.py
       |-- hf_infer.py

     -- eval
       |-- run_eval.py
       |-- metrics.py
       |-- evaluate_100.sh
       |-- data.py

     -- utils
       |-- post_processing.py
       |-- ref_normalize.py
       |-- prompting.py

     -- tools
       |-- convert_to_gguf.sh
       |-- merge.py

   -- compare_cli.py
   -- evaluate_100.sh
```

## GPU vs CPU Models
   This repository provides two ways to run the model:
   
   * *GPU (HF Transformers, merged model)*
   
     * Best performance (accuracy + speed).
      
     * Requires CUDA and enough VRAM.
      
     * Recommended for production use.
   
   * *CPU (Quantized GGUF via llama.cpp)*
   
     * Lower performance due to quantization + CPU-only execution.
      
     * Enables running the model in lightweight environments.
      
     * Used for the Hugging Face Space demo, which is CPU-only.
   
‚ö†Ô∏è The HF Space demo is much slower and slightly less accurate. For original performance, run the GPU merged model locally instead. The CPU model is for the demo-only.

## ‚ö° Quick Start

1. **Environment**

```
  conda create -n redaction python=3.11
  conda activate redaction pip install -r requirements.txt
```

2. **Preprocess & Train**
 
```
   bash scripts/train/train_full.sh
```

3. **Merge LoRA**

```
   python tools/merge.py
```

4. **Convert & Quantize**

```
   git clone https://github.com/ggerganov/llama.cpp cd llama.cpp && cmake -B build && cmake --build build -j && cd ..
   python llama.cpp/convert_hf_to_gguf.py outputs/pii_masking_mistral/merged_pii_model --outfile merged-gguf/mistral7b-redact-f16.gguf
   llama.cpp/build/bin/quantize merged-gguf/mistral7b-redact-f16.gguf merged-gguf/mistral7b-redact-Q4_K_M.gguf Q4_K_M
```

5. **Inference**

   * GPU:
```
   python apps/hf_demo.py
```
   * CPU (Quantized GGUF):
```
   python apps/pii_app.py
```

6. **Evaluation**

```
   bash evaluate_100.sh
```

## ‚ú® Example

```
   * Input: John Smith lives at 123 Main Street, Toronto. His credit card number is 4532 9483 0294 5521.
   * Output: [FIRSTNAME] [LASTNAME] lives at [ADDRESS]. His credit card number is [MASKEDNUMBER].
```

## üìä Results (100-sample evaluation)

*Will be updated soon ...*

## üõ† Post-Processing Rules

   * Canonicalize tags* ‚Üí unify casing/variants

   * Collapse addresses* ‚Üí [BUILDINGNUMBER] [STREET] [CITY] ‚Üí [ADDRESS]

   * Credit card override* ‚Üí detect 13‚Äì19 digit sequences and normalize to [MASKEDNUMBER]

   * Reduce tag space* ‚Üí CITY/STATE ‚Üí [ADDRESS]

## üó∫ Roadmap

   * [x] Fine-tune on PII dataset
   
   * [x] Merge LoRA ‚Üí base model
   
   * [x] Quantize with llama.cpp
   
   * [x] Post-processing & normalization

   * [x] HF Space live demo on CPU and local run on GPU
   
   * [ ] Eval on 100 samples
   
   

## üôè Credits

Dataset: ai4privacy/pii-masking-200k

Base model: mistralai/Mistral-7B-Instruct-v0.2

Tools: Axolotl
, llama.cpp

## üìú License

MIT License.
