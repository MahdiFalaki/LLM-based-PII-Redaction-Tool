# Base model & tokenizer
base_model: mistralai/Mistral-7B-Instruct-v0.2
model_type: MistralForCausalLM
tokenizer_type: LlamaTokenizer
trust_remote_code: false

# Datasets
datasets:
  - path: examples/pii_masking/data/pii_mask.jsonl
    type: alpaca
    message_property_mappings:
      system: system
      instruction: instruction
      input: input
      output: output

# Preprocess (tokenization/packing)
sequence_len: 1024
sample_packing: true
pad_to_sequence_len: true
preprocessed_data_dir: preprocessed/pii_masking_mistral  # <â€” easy to clean later

# Training output
output_dir: outputs/pii_masking_mistral

# Efficient finetune (LoRA)
adapter: lora
lora_r: 32
lora_alpha: 16
lora_dropout: 0.05
lora_target_linear: true

# Optim & schedule
learning_rate: 0.0002
optimizer: adamw_bnb_8bit
micro_batch_size: 1
gradient_accumulation_steps: 8
logging_steps: 1

# Memory savers
gradient_checkpointing: true
bf16: auto
tf32: false

# Misc
load_in_4bit: false
